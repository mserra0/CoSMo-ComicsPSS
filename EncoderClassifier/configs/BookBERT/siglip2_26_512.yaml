project: "BookBERT"
name: "siglip2"
config:
  lr: 1e-5
  architecture: "ClipLPbase"
  dataset: "DCM 400"
  epochs: 200
  dropout: 0.4
  batch_size: 64
  model_id: "google/siglip2-large-patch16-512"
  seed: 10
  augmentations: True
  num_aug_copies: 5
  num_synth_books: 1000
  num_attention_heads: 4
  num_hidden_layers: 4
  positional_embeddings: "absolute"
  hidden_dim: 256
  warmup: 44
  initial_lr: 1e-6