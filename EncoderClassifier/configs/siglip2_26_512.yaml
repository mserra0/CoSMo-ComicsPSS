project: "BookBERT"
name: "siglip2"
config:
  lr: 1e-3
  architecture: "ClipLPbase"
  dataset: "DCM small"
  epochs: 200
  dropout: 0.2
  batch_size: 32
  model_id: "google/siglip2-large-patch16-512"
  seed: 10
  augmentations: false
  num_aug_copies: 4
  num_synth_books: 500
  num_attention_heads: 4
  num_hidden_layers: 4
  positional_embeddings: "absolute"
  hidden_dim: 256
  warmup: 44
  initial_lr: 1e-6