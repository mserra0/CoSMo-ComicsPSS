{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import json\n",
    "import PSS_analyzer as pssa\n",
    "from sklearn.model_selection import train_test_split, cross_validate, StratifiedKFold, GridSearchCV, learning_curve\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import *\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from joblib import dump, load\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "\n",
    "annots_path = {\n",
    "    'train' : '/home-local/mserrao/PSSComics/multimodal-comic-pss/data/DCM/magi/train.json',\n",
    "    'val' : '/home-local/mserrao/PSSComics/multimodal-comic-pss/data/DCM/magi/val.json',\n",
    "    'test' : '/home-local/mserrao/PSSComics/multimodal-comic-pss/data/DCM/magi/test.json'\n",
    "}\n",
    "\n",
    "img_path = '/home-local/mserrao/PSSComics/multimodal-comic-pss/datasets.unify/DCM/images'\n",
    "\n",
    "splits = {\n",
    "    'train': '/home-local/mserrao/PSSComics/multimodal-comic-pss/datasets.unify/DCM/splits/train.csv',\n",
    "    'val' : '/home-local/mserrao/PSSComics/multimodal-comic-pss/datasets.unify/DCM/splits/val.csv',\n",
    "    'test' : '/home-local/mserrao/PSSComics/multimodal-comic-pss/datasets.unify/DCM/splits/test.csv'\n",
    "}\n",
    "\n",
    "GT_path = {\n",
    "    'train' : '/home-local/mserrao/PSSComics/multimodal-comic-pss/EncoderClassifier/data/comics_train.json',\n",
    "    'val' : '/home-local/mserrao/PSSComics/multimodal-comic-pss/EncoderClassifier/data/comics_val.json',\n",
    "    'test' : '/home-local/mserrao/PSSComics/multimodal-comic-pss/EncoderClassifier/data/comics_test.json'\n",
    "}\n",
    "\n",
    "CLS_MAPPING = {\n",
    "    1: 'Panel',\n",
    "    2: 'Character',\n",
    "    4: 'Text',\n",
    "    7: 'Face'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_annots(path):\n",
    "    with open(path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    images_df = pd.DataFrame(data['images'])\n",
    "    annotations_df = pd.DataFrame(data['annotations'])\n",
    "    \n",
    "    unique_image_ids = annotations_df['image_id'].unique()\n",
    "    image_id_to_page = {img_id: page_num for page_num, img_id in enumerate(unique_image_ids, start=1)}\n",
    "\n",
    "    annotations_df['page_number'] = annotations_df['image_id'].map(image_id_to_page)\n",
    "    \n",
    "    return images_df, annotations_df\n",
    "\n",
    "def load_GT(path):\n",
    "    with open(path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GT_data = load_GT(GT_path['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pssa.analyze_comic_book(annots_path['train'], plot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_stats = results['page_stats']\n",
    "annots_df = results['annotations']\n",
    "imgs_df = results['images']\n",
    "\n",
    "display(page_stats, imgs_df, annots_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "advanced_features = pssa.calculate_advanced_features(annots_df, imgs_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "advanced_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. **Panel-Related Features**:\n",
    "   - **Panel_bbox_max_dim_max**: Maximum of the maximum bounding box dimensions across panels.\n",
    "   - **Panel_bbox_max_dim_mean**: Mean of the maximum bounding box dimensions across panels.\n",
    "   - **Panel_area_max**: Maximum area of the panels.\n",
    "   - **Panel_area_mean**: Mean area of the panels.\n",
    "   - **Panel_bbox_count**: Number of bounding boxes (panels) on the page.\n",
    "   - **max_panel_centroid_x**: Maximum x-coordinate of the panel centroids.\n",
    "   - **max_panel_centroid_y**: Maximum y-coordinate of the panel centroids.\n",
    "   - **panel_coverage**: Coverage of the page by panels (likely a ratio of panel area to page area).\n",
    "\n",
    "### 2. **Character-Related Features**:\n",
    "   - **Character_bbox_max_dim_max**: Maximum of the maximum bounding box dimensions across characters.\n",
    "   - **Character_bbox_max_dim_mean**: Mean of the maximum bounding box dimensions across characters.\n",
    "   - **total_character_area**: Total area covered by character bounding boxes.\n",
    "   - **character_to_text_ratio**: Ratio of character area to text area.\n",
    "\n",
    "### 3. **Text-Related Features**:\n",
    "   - **total_text_area**: Total area covered by text bounding boxes.\n",
    "   - **text_to_panel_ratio**: Ratio of text area to panel area.\n",
    "   - **max_text_centroid_x**: Maximum x-coordinate of the text centroids.\n",
    "   - **max_text_centroid_y**: Maximum y-coordinate of the text centroids.\n",
    "\n",
    "### 4. **Face-Related Features**:\n",
    "   - **total_face_area**: Total area covered by face bounding boxes (if any).\n",
    "\n",
    "### 5. **Page-Level Features**:\n",
    "   - **page_number**: The page number within the book.\n",
    "   - **page_number_book**: The page number within the entire book.\n",
    "   - **page_type**: Type of page (e.g., \"story\").\n",
    "\n",
    "### 6. **Derived Ratios and Metrics**:\n",
    "   - **text_to_panel_ratio**: Ratio of text area to panel area.\n",
    "   - **character_to_text_ratio**: Ratio of character area to text area.\n",
    "   - **panel_coverage**: Coverage of the page by panels.\n",
    "\n",
    "### 7. **Centroid Features**:\n",
    "   - **max_panel_centroid_x**: Maximum x-coordinate of the panel centroids.\n",
    "   - **max_panel_centroid_y**: Maximum y-coordinate of the panel centroids.\n",
    "   - **max_text_centroid_x**: Maximum x-coordinate of the text centroids.\n",
    "   - **max_text_centroid_y**: Maximum y-coordinate of the text centroids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def add_page_type_annotations(page_stats, annotations_df, gt_data):\n",
    "#     page_type_map = {}\n",
    "    \n",
    "#     for book in gt_data:\n",
    "#         book_hash = book[\"hash_code\"]\n",
    "        \n",
    "#         for story in book.get(\"stories\", []):\n",
    "#             for page in range(story[\"page_start\"], story[\"page_end\"] + 1):\n",
    "#                 page_type_map[(book_hash, page)] = \"story\"\n",
    "        \n",
    "#         for textstory in book.get(\"textstories\", []):\n",
    "#             for page in range(textstory[\"page_start\"], textstory[\"page_end\"] + 1):\n",
    "#                 page_type_map[(book_hash, page)] = \"textstory\"\n",
    "        \n",
    "#         for ad in book.get(\"advertisements\", []):\n",
    "#             for page in range(ad[\"page_start\"], ad[\"page_end\"] + 1):\n",
    "#                 page_type_map[(book_hash, page)] = \"advertisement\"\n",
    "        \n",
    "#         for cover in book.get(\"covers\", []):\n",
    "#             for page in range(cover[\"page_start\"], cover[\"page_end\"] + 1):\n",
    "#                 page_type_map[(book_hash, page)] = \"cover\"\n",
    "    \n",
    "#     image_to_type = {}\n",
    "#     for _, row in annotations_df[['page_number', 'book_hash', 'page_number_book']].iterrows():\n",
    "#         book_hash = row['book_hash']\n",
    "#         page_num = row['page_number_book']  \n",
    "#         page_type = page_type_map.get((book_hash, page_num), \"unknown\")\n",
    "#         image_to_type[row['page_number']] = page_type\n",
    "    \n",
    "#     page_stats['page_type'] = page_stats['page_number'].map(image_to_type)\n",
    "    \n",
    "#     return page_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_page_type_annotations(page_stats, annotations_df, gt_data):\n",
    "    page_type_map = {}\n",
    "    first_page_story_map = {}  # Track first pages of stories\n",
    "    \n",
    "    for book in gt_data:\n",
    "        book_hash = book[\"hash_code\"]\n",
    "        \n",
    "        # Mark story pages and identify first pages\n",
    "        for story in book.get(\"stories\", []):\n",
    "            first_page = story[\"page_start\"]  # First page of this story\n",
    "            first_page_story_map[(book_hash, first_page)] = True\n",
    "            \n",
    "            for page in range(story[\"page_start\"], story[\"page_end\"] + 1):\n",
    "                page_type_map[(book_hash, page)] = \"story\"\n",
    "        \n",
    "        for textstory in book.get(\"textstories\", []):\n",
    "            for page in range(textstory[\"page_start\"], textstory[\"page_end\"] + 1):\n",
    "                page_type_map[(book_hash, page)] = \"textstory\"\n",
    "        \n",
    "        for ad in book.get(\"advertisements\", []):\n",
    "            for page in range(ad[\"page_start\"], ad[\"page_end\"] + 1):\n",
    "                page_type_map[(book_hash, page)] = \"advertisement\"\n",
    "        \n",
    "        for cover in book.get(\"covers\", []):\n",
    "            for page in range(cover[\"page_start\"], cover[\"page_end\"] + 1):\n",
    "                page_type_map[(book_hash, page)] = \"cover\"\n",
    "    \n",
    "    image_to_type = {}\n",
    "    for _, row in annotations_df[['page_number', 'book_hash', 'page_number_book']].iterrows():\n",
    "        book_hash = row['book_hash']\n",
    "        page_num = row['page_number_book']  \n",
    "        \n",
    "        # Check if this is a first page of a story\n",
    "        if first_page_story_map.get((book_hash, page_num), False):\n",
    "            image_to_type[row['page_number']] = \"story_first_page\"\n",
    "        else:\n",
    "            page_type = page_type_map.get((book_hash, page_num), \"unknown\")\n",
    "            image_to_type[row['page_number']] = page_type\n",
    "    \n",
    "    page_stats['page_type'] = page_stats['page_number'].map(image_to_type)\n",
    "    \n",
    "    return page_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_stats = add_page_type_annotations(page_stats, annots_df, GT_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_stats['page_type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_to_book = annots_df[['page_number', 'book_hash']].drop_duplicates().set_index('page_number')['book_hash']\n",
    "    \n",
    "page_stats['book_hash'] = page_stats['page_number'].map(page_to_book)\n",
    "\n",
    "display(page_stats.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_book_stats(page_stats, book_hash=None):\n",
    "    if not book_hash:\n",
    "        book_hash = page_stats['book_hash'].unique()[0]\n",
    "        \n",
    "    book = page_stats[page_stats['book_hash'] == book_hash]\n",
    "    \n",
    "    return book\n",
    "\n",
    "def plot_books_stats(page_stats, top_features, start=0, end=3, legend_position='outside'):\n",
    "    \"\"\"\n",
    "    Plot book statistics with improved legend placement.\n",
    "    \n",
    "    Args:\n",
    "        page_stats: DataFrame containing page statistics\n",
    "        top_features: List of metrics to display\n",
    "        start: Start index for book_hashes list\n",
    "        end: End index for book_hashes list\n",
    "        legend_position: Where to place legend ('outside', 'bottom', or 'default')\n",
    "    \"\"\"\n",
    "    book_hashes = page_stats['book_hash'].unique()[start:end]\n",
    "    for hash in book_hashes:\n",
    "        page_stats_sorted = get_book_stats(page_stats, book_hash=hash).sort_values(by='page_number_book', ascending=True)\n",
    "        \n",
    "        # Create figure with adjusted height for bottom legend if needed\n",
    "        if legend_position == 'bottom':\n",
    "            fig = plt.figure(figsize=(15, 14))  # Extra height for legend at bottom\n",
    "        else:\n",
    "            fig = plt.figure(figsize=(15, 12))\n",
    "            \n",
    "        # Call modified plot_book_stats with legend_position parameter\n",
    "        pssa.plot_book_stats(page_stats_sorted[:100], top_k_features=top_features, legend_position=legend_position)\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(page_stats, advanced_features):    \n",
    "    df = page_stats.copy()\n",
    "    df['category_name'] = df['category_id'].map(CLS_MAPPING)\n",
    "\n",
    "    features = ['bbox_max_dim_max', 'bbox_max_dim_mean', 'area_max', 'area_mean', 'bbox_count']\n",
    "    \n",
    "    unique_pages = df[['page_number', 'page_number_book', 'page_type']].drop_duplicates().reset_index(drop=True)\n",
    "    transformed_df = unique_pages.copy()\n",
    "\n",
    "    for category_id, category_name in CLS_MAPPING.items():\n",
    "        for feature in features:\n",
    "            category_data = df[df['category_id'] == category_id].copy()\n",
    "            if not category_data.empty:\n",
    "                pivot_data = category_data.set_index('page_number')[feature]\n",
    "                new_column = f\"{category_name}_{feature}\"\n",
    "                transformed_df[new_column] = transformed_df['page_number'].map(pivot_data)\n",
    "    \n",
    "    transformed_df = transformed_df.fillna(0)\n",
    "    \n",
    "    merged_df = pd.merge(\n",
    "        transformed_df,\n",
    "        advanced_features,\n",
    "        on=['page_number', 'page_number_book'],\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    print(f\"Original page_stats shape: {page_stats.shape}\")\n",
    "    print(f\"Advanced features shape: {advanced_features.shape}\")\n",
    "    print(f\"Transformed shape before merge: {transformed_df.shape}\")\n",
    "    print(f\"Final combined shape: {merged_df.shape}\")\n",
    "    display(merged_df.head())\n",
    "\n",
    "    return merged_df[merged_df['page_type'] != 'unknown']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_books_stats(page_stats, top_features=None, start=3, end=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GT_test = load_GT(GT_path['test'])\n",
    "results_test = pssa.analyze_comic_book(annots_path['test'], plot=False)\n",
    "\n",
    "page_stats_test = results_test['page_stats']\n",
    "annots_df_test = results_test['annotations']\n",
    "imgs_df_test = results_test['images']\n",
    "\n",
    "advanced_features_test = pssa.calculate_advanced_features(annots_df_test, imgs_df_test)\n",
    "\n",
    "page_stats_test = add_page_type_annotations(page_stats_test, annots_df_test, GT_test)\n",
    "\n",
    "page_to_book = annots_df_test[['page_number', 'book_hash']].drop_duplicates().set_index('page_number')['book_hash']\n",
    "    \n",
    "page_stats_test['book_hash'] = page_stats_test['page_number'].map(page_to_book)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = prepare_data(page_stats, advanced_features)\n",
    "test = prepare_data(page_stats_test, advanced_features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_importance(transformed_df, top_n=20):\n",
    "    df_copy = transformed_df.copy()\n",
    "    columns_to_drop = ['book_hash', 'page_number', 'page_number_book']\n",
    "    df_copy = df_copy.drop(columns=[col for col in columns_to_drop if col in df_copy.columns])\n",
    "\n",
    "    le = LabelEncoder()\n",
    "    df_copy['page_type_encoded'] = le.fit_transform(df_copy['page_type'])\n",
    "    df_copy = df_copy.drop(columns=['page_type'])\n",
    "\n",
    "    correlation = df_copy.corr()['page_type_encoded'].abs().sort_values(ascending=False)\n",
    "    correlation = correlation[correlation.index != 'page_type_encoded']\n",
    "    \n",
    "    correlation = correlation.head(top_n)\n",
    "\n",
    "    plt.figure(figsize=(14, 10))\n",
    "    \n",
    "    ax = sns.barplot(x=correlation.values, y=correlation.index, palette='viridis')\n",
    "    \n",
    "    plt.title('Top Features by Correlation with Page Type', fontsize=16)\n",
    "    plt.xlabel('Absolute Correlation', fontsize=14)\n",
    "    plt.ylabel('Features', fontsize=14)\n",
    "    \n",
    "    for i, v in enumerate(correlation.values):\n",
    "        ax.text(v + 0.01, i, f\"{v:.3f}\", va='center')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = plot_feature_importance(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "\n",
    "X_train = train.drop(['page_number', 'page_number_book', 'page_type', 'book_hash'], axis=1)\n",
    "y_train = train['page_type']\n",
    "\n",
    "dump(X_train, 'X_train.joblib')\n",
    "dump(y_train, 'y_train.joblib')\n",
    "\n",
    "# y_train_binary = (y_train == 'story').astype(int)\n",
    "y_train_binary = train['page_type'].isin(['story', 'story_first_page']).astype(int)\n",
    "y_train_multi = le.fit_transform(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test.drop(['page_number', 'page_number_book', 'page_type', 'book_hash'], axis=1)\n",
    "y_test = test['page_type']\n",
    "\n",
    "# y_test_binary = (y_test == 'story').astype(int)\n",
    "y_test_binary = test['page_type'].isin(['story', 'story_first_page']).astype(int)\n",
    "y_test_multi = le.fit_transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_mapping = dict(zip(le.classes_, range(len(le.classes_))))\n",
    "print(\"Class mapping:\")\n",
    "for class_name, encoded_value in classes_mapping.items():\n",
    "    print(f\"  {class_name} → {encoded_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grids = {\n",
    "    'logistic': {\n",
    "        'scaler': [StandardScaler(), RobustScaler(), MinMaxScaler()],\n",
    "        'classifier__C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "        'classifier__penalty': ['l1', 'l2', 'elasticnet', None],\n",
    "        'classifier__solver': ['saga', 'liblinear'],\n",
    "        'classifier__class_weight': ['balanced', None],\n",
    "        'classifier__max_iter': [1000]\n",
    "    },\n",
    "    \n",
    "    'random_forest': {\n",
    "        'scaler': [StandardScaler(), 'passthrough'], \n",
    "        'classifier__n_estimators': [50, 100, 200],\n",
    "        'classifier__max_depth': [None, 10, 20, 30],\n",
    "        'classifier__min_samples_split': [2, 5, 10],\n",
    "        'classifier__min_samples_leaf': [1, 2, 4],\n",
    "        'classifier__class_weight': ['balanced', 'balanced_subsample', None]\n",
    "    },\n",
    "    \n",
    "    'xgboost': {\n",
    "        'scaler': [StandardScaler(), 'passthrough'],\n",
    "        'classifier__n_estimators': [50, 100, 200],\n",
    "        'classifier__learning_rate': [0.01, 0.1, 0.3],\n",
    "        'classifier__max_depth': [3, 5, 7],\n",
    "        'classifier__min_child_weight': [1, 3, 5],\n",
    "        'classifier__subsample': [0.7, 0.8, 0.9]\n",
    "    },\n",
    "    \n",
    "    'svm': {\n",
    "        'scaler': [StandardScaler(), RobustScaler()],\n",
    "        'classifier__C': [0.1, 1, 10, 100],\n",
    "        'classifier__gamma': ['scale', 'auto', 0.1, 0.01],\n",
    "        'classifier__kernel': ['rbf', 'linear', 'poly'],\n",
    "        'classifier__class_weight': ['balanced', None]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate_models(pipes, X, y, cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42), scoring={'f1_macro': 'f1_macro'}):\n",
    "    res = []\n",
    "    for model_name, pipe in pipes.items():\n",
    "        results = cross_validate(pipe, X, y, cv=cv, scoring=scoring)\n",
    "        print(f\"\\n{model_name} Results:\")\n",
    "        for metric in scoring:\n",
    "            print(f\"{metric}: {np.mean(results[f'test_{metric}']):.4f} ± {np.std(results[f'test_{metric}']):.4f}\")\n",
    "        res.append(results)\n",
    "    return res\n",
    "\n",
    "def train_models(pipes, X_train, y_train):\n",
    "    for model_name, pipe in pipes.items():\n",
    "        print(f'Fitting {model_name}...')\n",
    "        pipe.fit(X_train, y_train)\n",
    "    \n",
    "def plot_confusion_matrix(models, X_test, y_test):\n",
    "    fig, axes = plt.subplots(1, len(models), figsize=(6*len(models), 5))\n",
    "    \n",
    "    if len(models) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for i, model in enumerate(models):\n",
    "        model_name = model.named_steps['classifier'].__class__.__name__\n",
    "        pred = model.predict(X_test)\n",
    "        cm = confusion_matrix(y_test, pred)\n",
    "        \n",
    "        # Normalize the confusion matrix by row (true label)\n",
    "        cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        \n",
    "        sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap='Blues',\n",
    "                   xticklabels=['Non-story', 'Story'],\n",
    "                   yticklabels=['Non-story', 'Story'], ax=axes[i])\n",
    "        \n",
    "        axes[i].set_title(f'{model_name} Normalized Confusion Matrix')\n",
    "        axes[i].set_xlabel('Predicted')\n",
    "        axes[i].set_ylabel('Actual')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "def grid_search_model(X_train, y_train, model_pipeline, param_grid, cv=5, scoring='f1_macro'):\n",
    "\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=model_pipeline,\n",
    "        param_grid=param_grid,\n",
    "        cv=cv,\n",
    "        scoring=scoring,\n",
    "        n_jobs=-1,\n",
    "        verbose=1,\n",
    "        return_train_score=True\n",
    "    )\n",
    "    \n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "    print(f\"Best cross-validation score: {grid_search.best_score_:.4f}\")\n",
    "    \n",
    "    return grid_search\n",
    "\n",
    "def evaluate_models(models, X_test, y_test):\n",
    "    results = {}\n",
    "    for name, model in models.items():\n",
    "        y_pred = model.predict(X_test)\n",
    "        class_report = classification_report(y_test, y_pred)\n",
    "        print(class_report)\n",
    "        y_prob = model.predict_proba(X_test)[:, 1]\n",
    "        \n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred)\n",
    "        recall = recall_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred, average='macro')\n",
    "        roc_auc = roc_auc_score(y_test, y_prob)\n",
    "        \n",
    "        results[name] = {\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1,\n",
    "            'roc_auc': roc_auc\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n{name} Test Results:\")\n",
    "        print(f\"  Accuracy:  {accuracy:.4f}\")\n",
    "        print(f\"  Precision: {precision:.4f}\")\n",
    "        print(f\"  Recall:    {recall:.4f}\")\n",
    "        print(f\"  F1 Score:  {f1:.4f}\")\n",
    "        print(f\"  ROC AUC:   {roc_auc:.4f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def plot_model_comparison(results):\n",
    "    \"\"\"Plot comparison of model performance\"\"\"\n",
    "    metrics = list(next(iter(results.values())).keys())\n",
    "    models = list(results.keys())\n",
    "    \n",
    "    fig, axes = plt.subplots(len(metrics), 1, figsize=(10, 3*len(metrics)))\n",
    "    \n",
    "    for i, metric in enumerate(metrics):\n",
    "        values = [results[model][metric] for model in models]\n",
    "        axes[i].bar(models, values)\n",
    "        axes[i].set_title(f'{metric.capitalize()}')\n",
    "        axes[i].set_ylim([0, 1])\n",
    "        axes[i].grid(axis='y', linestyle='--', alpha=0.7)\n",
    "        \n",
    "        for j, v in enumerate(values):\n",
    "            axes[i].text(j, v + 0.02, f'{v:.3f}', ha='center')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "def plot_learning_curve(estimator, X, y, title=None, ylim=None, cv=5,\n",
    "                        n_jobs=-1, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    if title is None:\n",
    "        if hasattr(estimator, 'named_steps') and 'classifier' in estimator.named_steps:\n",
    "            title = f\"Learning Curves for {estimator.named_steps['classifier'].__class__.__name__}\"\n",
    "        else:\n",
    "            title = \"Learning Curves\"\n",
    "            \n",
    "    plt.title(title)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "        \n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    \n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X, y, cv=cv, n_jobs=n_jobs, \n",
    "        train_sizes=train_sizes, scoring='f1_macro')\n",
    "        \n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    \n",
    "    plt.grid()\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1, color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\", label=\"Validation score\")\n",
    "    \n",
    "    plt.legend(loc=\"best\")\n",
    "    return plt\n",
    "\n",
    "def plot_full_learning_curve(estimator, X, y, title=None, cv=5,\n",
    "                            n_jobs=-1, train_sizes=np.linspace(.1, 1.0, 15)):\n",
    "    \"\"\"\n",
    "    Plot learning curve with full y-axis range from 0 to 1\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    if title is None:\n",
    "        if hasattr(estimator, 'named_steps') and 'classifier' in estimator.named_steps:\n",
    "            title = f\"Learning Curves for {estimator.named_steps['classifier'].__class__.__name__}\"\n",
    "        else:\n",
    "            title = \"Learning Curves\"\n",
    "            \n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.ylim([0.0, 1.01])  # Force y-axis to show full range\n",
    "        \n",
    "    plt.xlabel(\"Training examples\", fontsize=14)\n",
    "    plt.ylabel(\"Score\", fontsize=14)\n",
    "    \n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X, y, cv=cv, n_jobs=n_jobs, \n",
    "        train_sizes=train_sizes, scoring='f1_macro')\n",
    "        \n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    \n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                    train_scores_mean + train_scores_std, alpha=0.1, color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                    test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", linewidth=2, label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\", linewidth=2, label=\"Validation score\")\n",
    "    \n",
    "    plt.legend(loc=\"best\", fontsize=12)\n",
    "    \n",
    "    # Add horizontal lines at important score thresholds\n",
    "    plt.axhline(y=0.5, color='gray', linestyle='--', alpha=0.5)\n",
    "    plt.axhline(y=0.75, color='gray', linestyle='--', alpha=0.5)\n",
    "    \n",
    "    return plt\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = np.unique(y_train_binary)\n",
    "class_weights = compute_class_weight('balanced', classes=classes, y=y_train_binary)\n",
    "class_weight_dict = dict(zip(classes, class_weights))\n",
    "\n",
    "negative_count = sum(y_train_binary == 0)\n",
    "positive_count = sum(y_train_binary == 1)\n",
    "scale_pos_weight = negative_count / positive_count if positive_count > 0 else 1.0\n",
    "\n",
    "# pipes = {\n",
    "#     'logistic': Pipeline([\n",
    "#         ('scaler', StandardScaler()),\n",
    "#         ('classifier', LogisticRegression(random_state=42, class_weight=class_weight_dict))\n",
    "#     ]),\n",
    "    \n",
    "#     'random_forest': Pipeline([\n",
    "#         ('scaler', 'passthrough'),\n",
    "#         ('classifier', RandomForestClassifier(random_state=42, class_weight=class_weight_dict))\n",
    "#     ]),\n",
    "    \n",
    "#     'xgboost': Pipeline([\n",
    "#         ('scaler', StandardScaler()),\n",
    "#         ('classifier', XGBClassifier(random_state=42, scale_pos_weight=scale_pos_weight))\n",
    "#     ]),\n",
    "    \n",
    "#     'svm': Pipeline([\n",
    "#         ('scaler', StandardScaler()),\n",
    "#         ('classifier', SVC(random_state=42, probability=True, class_weight=class_weight_dict))\n",
    "#     ])\n",
    "# }\n",
    "\n",
    "\n",
    "# scoring = {\n",
    "#     'accuracy': 'accuracy',\n",
    "#     'precision_macro': 'precision_macro',\n",
    "#     'recall_macro': 'recall_macro',\n",
    "#     'f1_macro': 'f1_macro',\n",
    "#     'roc_auc': 'roc_auc'\n",
    "# }\n",
    "\n",
    "# cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "# res = cross_validate_models(pipes, X_train, y_train_binary, cv=cv, scoring=scoring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_models = {}\n",
    "\n",
    "# for model_name, pipeline in pipes.items():\n",
    "#     print(f\"\\n{'='*50}\")\n",
    "#     print(f\"Tuning {model_name}...\")\n",
    "#     print(f\"{'='*50}\")\n",
    "    \n",
    "#     grid_result = grid_search_model(\n",
    "#         X_train, \n",
    "#         y_train_binary, \n",
    "#         pipeline, \n",
    "#         param_grids[model_name],\n",
    "#         cv=cv, \n",
    "#         scoring='f1_macro'\n",
    "#     )\n",
    "    \n",
    "#     best_models[model_name] = grid_result.best_estimator_\n",
    "\n",
    "# dump(best_models, 'best_models.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_models = load('best_models.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = evaluate_models(best_models, X_test, y_test_binary)\n",
    "\n",
    "plot_confusion_matrix(list(best_models.values()), X_test, y_test_binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_name = max(test_results, key=lambda x: test_results[x]['f1'])\n",
    "best_model = best_models[best_model_name]\n",
    "plot_learning_curve(best_model, X_train, y_train_binary, train_sizes=np.linspace(.1, 1.0, 15))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = best_model.named_steps['classifier'].get_params()\n",
    "\n",
    "print(\"\\nBest model structure:\")\n",
    "print(best_model)\n",
    "print(params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model_feature_importance(model, feature_names=None, top_n=None):\n",
    "    importances = model.feature_importances_\n",
    "    \n",
    "    if feature_names is None:\n",
    "        feature_names = [f\"Feature {i}\" for i in range(len(importances))]\n",
    "    \n",
    "    indices = np.argsort(importances)[::-1]\n",
    "    if top_n:\n",
    "        indices = indices[:top_n]\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.title(\"Feature Importance\")\n",
    "    plt.bar(range(len(indices)), importances[indices], color=\"b\", align=\"center\")\n",
    "    plt.xticks(range(len(indices)), [feature_names[i] for i in indices], rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_feature_importance(best_model.named_steps['classifier'], feature_names=X_train.columns, top_n=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiclass_evaluate_models(models, X_test, y_test, class_names):\n",
    "    results = {}\n",
    "    for name, model in models.items():\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred, average='macro')\n",
    "        recall = recall_score(y_test, y_pred, average='macro')\n",
    "        f1 = f1_score(y_test, y_pred, average='macro')\n",
    "        \n",
    "        results[name] = {\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n{name} Test Results:\")\n",
    "        print(f\"  Accuracy:  {accuracy:.4f}\")\n",
    "        print(f\"  Precision: {precision:.4f}\")\n",
    "        print(f\"  Recall:    {recall:.4f}\")\n",
    "        print(f\"  F1 Score:  {f1:.4f}\")\n",
    "        \n",
    "        print(\"\\nPer-class metrics:\")\n",
    "        class_report = classification_report(y_test, y_pred, target_names=class_names)\n",
    "        print(class_report)\n",
    "    \n",
    "    return results\n",
    "\n",
    "def plot_multiclass_confusion_matrix(models, X_test, y_test, class_names):\n",
    "    fig, axes = plt.subplots(1, len(models), figsize=(7*len(models), 6))\n",
    "    \n",
    "    if len(models) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for i, (model_name, model) in enumerate(models.items()):\n",
    "        y_pred = model.predict(X_test)\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        \n",
    "        # Normalize the confusion matrix by row (true label)\n",
    "        cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        \n",
    "        sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap='Blues',\n",
    "                   xticklabels=class_names,\n",
    "                   yticklabels=class_names, ax=axes[i])\n",
    "        \n",
    "        axes[i].set_title(f'{model_name} Normalized Confusion Matrix')\n",
    "        axes[i].set_xlabel('Predicted')\n",
    "        axes[i].set_ylabel('Actual')\n",
    "        plt.setp(axes[i].get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "def create_multiclass_pipelines(X_train, y_train):\n",
    "    \n",
    "    n_classes = len(np.unique(y_train))\n",
    "    \n",
    "    classes = np.unique(y_train)\n",
    "    class_weights = compute_class_weight('balanced', classes=classes, y=y_train)\n",
    "    class_weight_dict = dict(zip(classes, class_weights))\n",
    "    \n",
    "    pipes = {\n",
    "        'logistic': Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('classifier', LogisticRegression(\n",
    "                random_state=42, \n",
    "                multi_class='multinomial',\n",
    "                solver='lbfgs',\n",
    "                class_weight=class_weight_dict,\n",
    "                max_iter=1000\n",
    "            ))\n",
    "        ]),\n",
    "        \n",
    "        'random_forest': Pipeline([\n",
    "            ('scaler', 'passthrough'),\n",
    "            ('classifier', RandomForestClassifier(\n",
    "                random_state=42, \n",
    "                class_weight=class_weight_dict\n",
    "            ))\n",
    "        ]),\n",
    "        \n",
    "        'xgboost': Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('classifier', XGBClassifier(\n",
    "                random_state=42,\n",
    "                objective='multi:softprob',\n",
    "                num_class=n_classes\n",
    "            ))\n",
    "        ]),\n",
    "        \n",
    "        'svm': Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('classifier', SVC(\n",
    "                random_state=42,\n",
    "                probability=True,\n",
    "                decision_function_shape='ovr',\n",
    "                class_weight=class_weight_dict\n",
    "            ))\n",
    "        ])\n",
    "    }\n",
    "    \n",
    "    return pipes\n",
    "\n",
    "def plot_model_feature_importance(model, feature_names, top_n=20):\n",
    "    \"\"\"\n",
    "    Plot feature importance for tree-based models with improved formatting.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained model with feature_importances_ attribute\n",
    "        feature_names: List of feature names\n",
    "        top_n: Number of top features to plot (default: 20)\n",
    "    \"\"\"\n",
    "    if hasattr(model, 'named_steps') and 'classifier' in model.named_steps:\n",
    "        if hasattr(model.named_steps['classifier'], 'feature_importances_'):\n",
    "            importances = model.named_steps['classifier'].feature_importances_\n",
    "        else:\n",
    "            print(\"Model doesn't have feature_importances_ attribute\")\n",
    "            return\n",
    "    elif hasattr(model, 'feature_importances_'):\n",
    "        importances = model.feature_importances_\n",
    "    else:\n",
    "        print(\"Model doesn't have feature_importances_ attribute\")\n",
    "        return\n",
    "    \n",
    "    indices = np.argsort(importances)[::-1]\n",
    "    if top_n:\n",
    "        indices = indices[:top_n]\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.title(\"Feature Importance\", fontsize=16)\n",
    "    \n",
    "    y_pos = np.arange(len(indices))\n",
    "    plt.barh(y_pos, importances[indices], align='center', color='skyblue')\n",
    "    \n",
    "    plt.yticks(y_pos, [feature_names[i] for i in indices])\n",
    "    \n",
    "    for i, v in enumerate(importances[indices]):\n",
    "        plt.text(v + 0.001, i, f\"{v:.3f}\", va='center')\n",
    "    \n",
    "    plt.xlabel(\"Importance\", fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiclass_pipes = create_multiclass_pipelines(X_train, y_train_multi)\n",
    "\n",
    "print(\"Cross-validating multiclass models...\")\n",
    "# multiclass_cv_results = cross_validate_models(multiclass_pipes, X_train, y_train_multi, \n",
    "#                                              scoring=scoring)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_models_multi = {}\n",
    "\n",
    "# for model_name, pipeline in multiclass_pipes.items():\n",
    "#     print(f\"\\n{'='*50}\")\n",
    "#     print(f\"Tuning {model_name}...\")\n",
    "#     print(f\"{'='*50}\")\n",
    "    \n",
    "#     grid_result = grid_search_model(\n",
    "#         X_train, \n",
    "#         y_train_multi, \n",
    "#         pipeline, \n",
    "#         param_grids[model_name],\n",
    "#         cv=cv, \n",
    "#         scoring='f1_macro'\n",
    "#     )\n",
    "    \n",
    "#     best_models_multi[model_name] = grid_result.best_estimator_\n",
    "\n",
    "# dump(best_models_multi, 'best_models_multi.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_models_multi = load('best_models_multi.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_models(best_models_multi, X_train, y_train_multi)\n",
    "\n",
    "class_names = list(le.classes_)\n",
    "multiclass_test_results = multiclass_evaluate_models(best_models_multi, X_test, y_test_multi, class_names)\n",
    "\n",
    "plot_multiclass_confusion_matrix(best_models_multi, X_test, y_test_multi, class_names)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "magi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
